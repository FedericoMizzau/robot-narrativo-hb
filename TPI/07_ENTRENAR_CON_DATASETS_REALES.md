# âœ… DATASETS PROCESADOS Y LISTOS PARA ENTRENAR

## ğŸ“Š Resumen del Procesamiento

### âœ… Datasets Procesados Exitosamente:

1. **merged_clean.txt**
   - âœ… 812 cuentos procesados

2. **fairy_tales/** (1651 archivos)
   - âœ… 1038 cuentos vÃ¡lidos procesados
   - âš ï¸  613 archivos no vÃ¡lidos (muy cortos o vacÃ­os)

3. **Total Final:**
   - âœ… **1,827 cuentos Ãºnicos**
   - âœ… **1,753,298 palabras totales**
   - âœ… **959 palabras promedio por cuento**
   - âœ… **8.99 MB de texto procesado**

### ğŸ“ Archivo Generado:
- `robot_narrativo/data/cuentos_procesados.txt` - Listo para entrenar

---

## ğŸš€ Entrenar el Modelo

### OpciÃ³n 1: Script AutomÃ¡tico (Recomendado)

```bash
cd robot_narrativo
source venv/Scripts/activate
python entrenar_modelo_cuentos.py
```

Este script:
- âœ… Detecta automÃ¡ticamente GPU/CPU
- âœ… Ajusta parÃ¡metros segÃºn hardware
- âœ… Muestra tiempo estimado
- âœ… Entrena con configuraciÃ³n optimizada

### OpciÃ³n 2: Manual

```bash
cd robot_narrativo
source venv/Scripts/activate

# Con GPU (si estÃ¡ disponible)
python fine_tuning.py data/cuentos_procesados.txt --epochs 3 --batch 4

# Solo CPU (mÃ¡s lento)
python fine_tuning.py data/cuentos_procesados.txt --epochs 2 --batch 2 --cpu
```

---

## â±ï¸ Tiempos Estimados

Con **1,827 cuentos**:

| Hardware | Ã‰pocas | Tiempo Estimado |
|----------|--------|-----------------|
| **GPU NVIDIA** | 3 | 2-3 horas |
| **CPU** | 2 | 6-8 horas |

**Nota:** Puedes reducir el nÃºmero de Ã©pocas o usar un subconjunto del dataset para entrenar mÃ¡s rÃ¡pido.

---

## ğŸ¯ Usar el Modelo Entrenado

Una vez completado el entrenamiento, el modelo se guardarÃ¡ en:
- `robot_narrativo/modelo_cuentos_entrenado/`

### Actualizar app.py:

```python
generador = GeneradorCuento(
    usar_api_openai=usar_openai, 
    api_key=OPENAI_API_KEY,
    usar_ml=True,
    modelo_ml="./modelo_cuentos_entrenado"  # â† Cambiar aquÃ­
)
```

---

## ğŸ“ Opciones de Entrenamiento

### Entrenamiento RÃ¡pido (para pruebas):
```bash
# Usar solo una muestra del dataset
head -n 10000 data/cuentos_procesados.txt > data/cuentos_muestra.txt
python fine_tuning.py data/cuentos_muestra.txt --epochs 1 --batch 4
```

### Entrenamiento Completo (mejor calidad):
```bash
python fine_tuning.py data/cuentos_procesados.txt --epochs 3 --batch 4
```

### Entrenamiento Extendido (mÃ¡xima calidad):
```bash
python fine_tuning.py data/cuentos_procesados.txt --epochs 5 --batch 4
```

---

## ğŸ” Verificar Progreso

Durante el entrenamiento verÃ¡s:
- Logs cada 50 pasos
- Checkpoints guardados cada 500 pasos
- PÃ©rdida (loss) que deberÃ­a disminuir

Si se interrumpe, el Ãºltimo checkpoint se guarda automÃ¡ticamente.

---

## âœ… Ventajas del Modelo Entrenado

### Antes (GPT-2 base):
- Genera principalmente en inglÃ©s
- Estilo genÃ©rico
- No conoce estructura de cuentos

### DespuÃ©s (Fine-tuned con tus cuentos):
- âœ… **Aprende el estilo de tus cuentos**
- âœ… **Mejor estructura narrativa**
- âœ… **MÃ¡s coherencia temÃ¡tica**
- âœ… **GeneraciÃ³n mÃ¡s creativa y variada**

---

## ğŸ“ PrÃ³ximos Pasos

1. **Entrenar el modelo:**
   ```bash
   python entrenar_modelo_cuentos.py
   ```

2. **Esperar a que complete** (puede tardar horas)

3. **Actualizar app.py** con la ruta del modelo entrenado

4. **Probar el sistema:**
   ```bash
   python app.py
   ```

5. **Comparar resultados:**
   - Generar cuentos con el modelo base
   - Generar cuentos con el modelo entrenado
   - Ver la diferencia en calidad

---

## ğŸ’¡ Consejos

- **Si tienes GPU:** Usa el entrenamiento completo (3 Ã©pocas)
- **Si solo CPU:** Considera 2 Ã©pocas o una muestra mÃ¡s pequeÃ±a
- **Para pruebas rÃ¡pidas:** Entrena con 100-200 cuentos primero
- **Para producciÃ³n:** Usa todos los cuentos con 3-5 Ã©pocas

---

## ğŸ“Š EstadÃ­sticas del Dataset

- **Total cuentos:** 1,827
- **Total palabras:** 1,753,298
- **Promedio palabras/cuento:** 959
- **TamaÃ±o archivo:** 8.99 MB
- **Calidad:** Excelente para fine-tuning

---

**Â¡El dataset estÃ¡ listo! Ahora puedes entrenar el modelo para generar cuentos mucho mejores! ğŸš€**

